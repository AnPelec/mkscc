{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('urop_dataset_training.csv')\n",
    "df_validation = pd.read_csv('urop_dataset_validation.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the report data\n",
    "\n",
    "### Idea 1\n",
    "##### 1. Breaking the paragraphs into different reports\n",
    "##### 2. Giving every paragraph the same class and train on that with bow classifier and ngrams\n",
    "##### 3. Prediction is made using an average?\n",
    "\n",
    "### Idea 2\n",
    "##### 1. Use the paragraphs as a whole for your training\n",
    "##### 2. Probably too long\n",
    "\n",
    "### Idea 3\n",
    "##### 1. Concatenate all the reports from the different visits\n",
    "##### 2. Even longer text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this field is the class we're trying to predict and hence we have to strip any whitespaces from it\n",
    "df_train[\"Objective Response per RECIST v1.1\"] = df_train[\"Objective Response per RECIST v1.1\"].apply(lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning scan report text\n",
    "df_train[\"clean_report_text\"] = df_train[\"Scan report text\"].apply(lambda text: re.sub('\\W+', ' ', text).lower().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_encode_column(df, column_name):\n",
    "    onehot_cols = pd.get_dummies(df[column_name])\n",
    "    df = df.drop(column_name, axis=1)\n",
    "    df = df.join(onehot_cols)\n",
    "    return df\n",
    "\n",
    "def days_after_start(row):\n",
    "    start_date = row[\"Treatment start date\"]\n",
    "    current_date = row[\"Date of scan\"]\n",
    "    return (datetime.strptime(current_date, '%m/%d/%Y') - datetime.strptime(start_date, '%m/%d/%Y')).days\n",
    "\n",
    "def clean_dataframe(df):\n",
    "    # treatment setting to one hot\n",
    "    df = onehot_encode_column(df, \"Treatment setting\")\n",
    "    # scan included on recist form to one hot\n",
    "    df = onehot_encode_column(df, \"Scan included on RECIST form? (y/n)\")\n",
    "    # type of scan to one hot\n",
    "    df = onehot_encode_column(df, \"Type of scan\")\n",
    "    # difference of dates\n",
    "    df[\"date_dist\"] = df[[\"Treatment start date\", \"Date of scan\"]].apply(days_after_start, axis=1)\n",
    "    # scan timepoint\n",
    "    scan_timepoint = \"Scan timepoint (baseline = prior to treatment start, ontx = during treatment or prior to progression if stopped treatment , progression = time of RECIST defined progression)\"\n",
    "    df[scan_timepoint] = df[scan_timepoint] == \"baseline\"\n",
    "    df[scan_timepoint] *= 1\n",
    "    df = df.drop([\"Patient ID\", \\\n",
    "                  \"PFS censor                              (1 = progressed, 0 = has not progressed)\", \\\n",
    "                  \"Treatment start date\", \"Date of scan\", \\\n",
    "                  \"Date of radiologic progression-free survival (PFS, calculated from start date)\", \\\n",
    "                  \"Scan type specified\", \\\n",
    "                  \"Objective Response per RECIST v1.1\", \\\n",
    "                  \"Scan report text\", \\\n",
    "                  \"clean_report_text\"], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train = clean_dataframe(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train_X = df_train[\"clean_report_text\"]\n",
    "text_train_y = df_train[\"Objective Response per RECIST v1.1\"]\n",
    "min_df = 2\n",
    "ngram_range = (1, 3)\n",
    "max_features = 10000\n",
    "label_enc = LabelEncoder()\n",
    "enc = OneHotEncoder()\n",
    "countVec = CountVectorizer(min_df = min_df, ngram_range = ngram_range, max_features = max_features)\n",
    "# Learn vocabulary from train set\n",
    "countVec.fit(text_train_X)\n",
    "# Transform list of review to matrix of bag-of-word vectors\n",
    "trainX = countVec.transform(text_train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainX.toarray().shape)\n",
    "trainX = np.hstack([trainX.toarray(), text_train.values])\n",
    "print(trainX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_enc_y = label_enc.fit(text_train_y.values)\n",
    "trainY = label_enc_y.transform(text_train_y.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(C=0.01, solver='saga')\n",
    "scores = cross_val_score(lr, trainX, trainY, cv=5)\n",
    "print(scores)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(C=0.01, class_weight='balanced', solver='sag', multi_class='multinomial')\n",
    "scores = cross_val_score(lr, trainX, trainY, cv=5)\n",
    "print(scores)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GradientBoostingClassifier(n_estimators=1000)\n",
    "#clf.fit(trainX, text_train_y_labels)\n",
    "scores = cross_val_score(clf, trainX, trainY, cv=5)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
